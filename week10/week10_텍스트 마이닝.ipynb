{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 10: 텍스트 마이닝**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 텍스트 전처리** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-1. 텍스트 전처리란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자연어로 작성된 데이터를 기계 학습 및 분석에 적합한 형식으로 정제(Cleaning)하고 구조화(Structuring)하는 작업임\n",
    "  \n",
    "  * 예: 특수문자 제거, 대소문자 통일, 단어 분리, 불용어 제거 등\n",
    "  \n",
    "  * 일반적으로 텍스트 데이터를 정형(structured) 데이터 형태로 바꾸기 위한 기초 단계를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-2. 자연어 처리 vs 텍스트 마이닝**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog-ko.superb-ai.com/content/images/size/w1000/2023/07/pasted-image-0.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 항목       | 자연어 처리 (NLP)                                  | 텍스트 마이닝 (Text Mining)                                 |\n",
    "|------------|----------------------------------------------------|--------------------------------------------------------------|\n",
    "| 정의       | 인간 언어를 이해하고 처리하는 기술                  | 텍스트 데이터에서 유용한 정보나 패턴을 추출하는 기술         |\n",
    "| 목적       | 언어의 구조와 의미를 분석하여 이해                  | 데이터로부터 인사이트와 지식을 발견                         |\n",
    "| 주요 기술  | 구문 분석, 의미 분석, 언어 모델링 등                | 텍스트 분류, 클러스터링, 감성 분석, 토픽 모델링 등           |\n",
    "| 응용 분야  | 챗봇, 음성 인식, 기계 번역 등                       | 문서 분류, 여론 분석, 기업 인사이트 추출 등                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-3. 텍스트 전처리 필요성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **데이터 일관성 유지**\n",
    "  * 다양한 출처에서 수집된 텍스트는 형식, 구조, 표현 방식이 제각기 다를 수 있음 → 모델이 데이터를 일관되게 학습하도록 유도할 수 있음\n",
    "    * 예: 대소문자 혼용, 특수 문자, 불필요한 공백 등\n",
    "  \n",
    "\n",
    "* **노이즈 제거**\n",
    "  * 텍스트에는 분석에 불필요하거나 방해가 되는 정보들이 포함될 수 있음 → 데이터 품질과 모델 성능을 향상시킬 수 있음\n",
    "    * 예: HTML 태그, 이모지, 특수문자, 광고 문구, 반복 문자, 오타 등\n",
    "\n",
    "\n",
    "* **중요 정보 강조**\n",
    "  * 불용어(stopwords) 제거, 어간 추출(stemming), 원형 복원(lemmatization) 등의 기법을 통해 텍스트 내 핵심 정보를 강조할 수 있음 → 분석의 정확도와 효율성을 높일 수 있음\n",
    "\n",
    "\n",
    "* **차원 축소**\n",
    "  * 원시 텍스트는 단어 수가 많아 벡터화 시 매우 고차원 데이터가 생성됨\n",
    "  \n",
    "  * 전처리를 통해 불필요한 단어를 제거하거나 단어를 통합함으로써 차원을 줄이고 계산 비용을 절감할 수 있음 → 모델의 학습 속도와 성능 개선에 기여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-4. 텍스트 전처리 vs 특성 추출**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 항목       | 텍스트 전처리                             | 특성 추출                             |\n",
    "|------------|--------------------------------------------|-----------------------------------------------|\n",
    "| 목적       | 모델 입력에 적합한 텍스트 정제             | 모델이 학습할 수 있는 유의미한 특성 생성     |\n",
    "| 처리 대상  | 단어, 문장, 특수기호, 문법적 형태 등       | 텍스트 임베딩, TF-IDF, 문장 길이 등          |\n",
    "| 예시       | `\"What is this???\" → \"what is this\"`       | BERT 임베딩 추출, 감성 점수 생성 등           |\n",
    "| 시점       | 모델 학습 전 단계                          | 전처리 이후, 특성 설계 시                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 텍스트 전처리 프로세스**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트 전처리는 텍스트 데이터를 분석 또는 머신러닝 모델링에 적합한 형태로 가공하는 과정이며, 일반적으로 다음의 3단계로 수행됨\n",
    "\n",
    "* 단계별 세부 작업은 프로젝트의 목적, 데이터의 특성, 분석 방향에 따라 조정하거나 생략될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-1. 데이터 정제 (Cleaning)**\n",
    "\n",
    "* 목적: 노이즈 제거 및 일관성 확보를 통해 텍스트 데이터의 품질을 향상\n",
    "\n",
    "* 주요 작업\n",
    "    \n",
    "    * 노이즈 제거: HTML 태그, 특수 문자, 불필요한 기호, 공백 등 제거\n",
    "\n",
    "    * 대소문자 변환: `\"Apple\"` → `\"apple\"`\n",
    "\n",
    "    * 축약어 확장: `\"don't\"` → `\"do not\"`\n",
    "\n",
    "    * 텍스트 정규화: `\"U.S.A.\"` → `\"USA\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-2. 데이터 변환 (Transformation)**\n",
    "\n",
    "* 목적: 텍스트를 분석/모델링에 적합한 형태로 구조화\n",
    "\n",
    "* 주요 작업\n",
    "    \n",
    "    * 토큰화(Tokenization): 문장을 단어 또는 문장 단위로 분할\n",
    "\n",
    "    * 불용어 제거(Stopword Removal): 의미 없는 단어 제거 (예: the, and, is 등)\n",
    "\n",
    "    * 어간 추출(Stemming) 또는 원형 복원(Lemmatization):\n",
    "\n",
    "        * `\"running\"` → `\"run\"`\n",
    "\n",
    "        * `\"better\"` → `\"good\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. 특성 추출 (Feature Extraction)**\n",
    "\n",
    "* 목적: 텍스트에서 모델 학습에 활용할 수 있는 수치형 특성(Feature) 추출\n",
    "\n",
    "* 주요 작업\n",
    "    \n",
    "    * 단어 빈도 기반 표현: One-hot Encoding, TF-IDF 등으로 단어의 중요도 수치화\n",
    "\n",
    "    * 단어 임베딩(Word Embedding): 단어를 벡터 공간상 위치로 표현 (예: Word2Vec, GloVe, BERT Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 텍스트 정제**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-1. HTML 태그 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹 크롤링 데이터에는 다양한한 HTML 태그가 포함되어 있으며,이는 텍스트 분석에는 불필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    <html>\n",
    "        <head><title>My Title</title></head> 😍😍\n",
    "        <body><p>Hello, World! &#128512;</p><p>Python is fun. <a href=\"http://example.com\">Example</a></p></body>\n",
    "    </html>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     \n",
      "          My Title   😍😍\n",
      "          Hello, World! &#128512;  Python is fun.  Example   \n",
      "     \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# re.sub 함수\n",
    "text_no_html = re.sub(r'<.*?>', ' ', text) #정규표현식으로 정리\n",
    "print(text_no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My Title 😍😍\n",
      "        Hello, World! 😀Python is fun. Example\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# BeautifulSoup\n",
    "text_no_html = BeautifulSoup(text, \"html.parser\").get_text() #이거는 정규 표현식으로 정리 안해도 됨\n",
    "print(text_no_html)  #쓸떄없는걸 없애줌 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.sub()` vs `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 항목             | `re.sub()`                                          | `BeautifulSoup`                                                  |\n",
    "|------------------|-----------------------------------------------------|------------------------------------------------------------------|\n",
    "| 주요 목적        | 정규표현식을 이용해 텍스트 내 패턴을 치환 또는 제거 | HTML/XML 구조를 파싱하여 요소를 추출하거나 제거                  |\n",
    "| 사용 방식        | 문자열 기반 처리                                     | 트리 구조 기반 처리 (DOM 탐색 가능)                              |\n",
    "| HTML 처리 적합성 | 복잡한 HTML은 처리하기 어려움                        | HTML 태그, 속성 등을 구조적으로 정확하게 처리 가능              |\n",
    "| 학습 난이도      | 정규표현식 문법 숙지가 필요                          | 비교적 직관적인 API (`.find()`, `.text` 등) 제공                 |\n",
    "| 속도             | 빠름 (단순 텍스트 치환 시)                          | 다소 느림 (HTML 파싱 포함)                                       |\n",
    "| 주요 활용 예     | 간단한 텍스트 정제 (예: 특수문자 제거, 이메일 수정) | HTML에서 텍스트 추출, 태그 제거, 속성 접근 등 웹 크롤링에 적합   |\n",
    "| 외부 라이브러리 필요 | 필요 없음 (`re`는 파이썬 내장 모듈)               | 필요 있음 (`bs4` 패키지 설치 필요: `pip install beautifulsoup4`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-2. 이모지 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이모지는 텍스트가 아닌 유니코드 특수문자이기 때문에 일반 NLP 처리 시 제거 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love it 😍😍!! Best product ever!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love it !! Best product ever!!!\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-3. 특수문자 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 부호나 기호 등은 일반적으로 분석에 도움이 되지 않으며, 단어 토큰화에 혼란을 줄 수 있음\n",
    "\n",
    "* 다만, 감성 분석 등 특정 상황에서는 유지할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I can't believe this!!! It's amazing :) #excited 😁😁\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cant believe this Its amazing  excited \n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-4. 숫자 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트 분석 목적에 따라 숫자는 의미가 없거나 오히려 노이즈가 될 수 있음\n",
    "\n",
    "* 예: 가격, 날짜, 숫자형 코드 등 제거 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This phone costs 999 dollars and was released in 2023.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This phone costs  dollars and was released in .\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'\\d+', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-5. 소문자 변환**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자연어 처리에서는 `'Apple'`과 `'apple'`을 의미적으로 동일한 단어로 취급해야 할 경우가 많음\n",
    "\n",
    "* 대소문자를 구분하지 않는 처리는 불필요한 단어 수 증가를 방지하고 학습 데이터의 일관성을 높여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is Looking at Buying Another Startup.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple is looking at buying another startup.\n"
     ]
    }
   ],
   "source": [
    "lowered = text.lower()\n",
    "print(lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-6. 공백 처리** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트에는 의도치 않은 여러 개의 공백, 탭, 줄바꿈 문자 등이 포함될 수 있음\n",
    "\n",
    "* 이러한 공백은 토큰화 또는 모델 입력 시 불필요한 토큰 생성을 유발할 수 있으므로 정리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "     \n",
    "          My Title  \n",
    "          Hello World   Python is fun  Example   \n",
    "     \n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Title Hello World Python is fun Example\n"
     ]
    }
   ],
   "source": [
    "text_clean = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-7. 축약어 확장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 축약어 확장은 `'don't'`를 `'do not'`으로, `'I'm'`을 `'I am'`으로 변환하는 등 축약된 단어 형태를 원래 형태로 확장하는 과정을 의미함\n",
    "\n",
    "* 축약형을 원형으로 확장하면 모든 텍스트가 동일한 형태를 갖게 되어 분석의 일관성이 향상함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install contractions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m   \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 축약형이 포함된 텍스트\u001b[39;00m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm here but I don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know what to do.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "#!pip install contractions\n",
    "import contractions   \n",
    "\n",
    "# 축약형이 포함된 텍스트\n",
    "text = \"I'm here but I don't know what to do.\"\n",
    "\n",
    "# 축약형 확장\n",
    "expanded_text = contractions.fix(text)\n",
    "\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 데이터 변환**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-1. 토큰화**\n",
    "\n",
    "* 문장을 의미 있는 단위인 단어(word), 형태소(morpheme), 서브워드(subword) 등으로 나누는 작업임\n",
    "\n",
    "* BoW, TF-IDF, 워드 임베딩 등 모두 토큰화를 기반으로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLTK (Natural Language Toolkit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLTK는 Python에서 자연어 처리를 수행할 수 있도록 도와주는 대표적인 오픈소스 라이브러리임\n",
    "\n",
    "* 토큰화(tokenization), 형태소 분석(pos tagging), 불용어 제거, 어간 추출 등 다양한 기능을 제공함\n",
    "\n",
    "* 실무에서 대용량 데이터를 처리하거나 속도가 중요한 경우에는 spaCy, Hugging Face Transformers가 더 적합할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')  \n",
    "# nltk.download('punkt_tab') # 토큰화 도구(최초 1회만 다운로드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UserK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is fun!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-2. 불용어 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장에서 자주 등장하지만 문맥상 의미가 거의 없거나 분석에 도움이 되지 않는 단어를 의미함\n",
    "\n",
    "* 예: `is`, `the`, `a`, `and`, `in`, `of`,` to`, `this`, `that` 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')   # 불용어 리스트(최초 1회만 다운로드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UserK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence with some stopwords.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **커스텀 불용어 사전 구축 방법**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용 도메인에 따라 기본 불용어 외에 추가/제거가 필요할 수 있음\n",
    "\n",
    "* 예: 금융 도메인에서 ‘bank’는 불용어가 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 불용어 리스트를 기반으로 커스터마이징\n",
    "custom_stopwords = set(stopwords.words('english'))\n",
    "custom_stopwords.remove('not')                     # 제거: 'not'은 분석에 중요할 수 있음\n",
    "custom_stopwords.update(['sample', 'sentence'])    # 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['than', \"you'll\", 'further', 'into', 'a', 'on', 'over', 'how', \"they'll\", \"it'll\", 'm', 'o', 'as', 'only', 'both', 'ma', 'whom', 'my', 'with', 'do']\n"
     ]
    }
   ],
   "source": [
    "# 일부 불용어 출력\n",
    "print(list(stop_words)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용\n",
    "filtered = [word for word in tokens if word not in custom_stopwords]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-3. 어간 추출 vs 표제어 추출**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 항목             | Stemming (어간 추출)                                  | Lemmatization (표제어 추출)                           |\n",
    "|------------------|--------------------------------------------------------|--------------------------------------------------------|\n",
    "| 정의             | \t단어의 접사 등을 규칙 기반으로 제거하여 어근 추출 | 문법적 품사와 의미를 고려하여 표제어(기본형)로 변환 |\n",
    "| 처리 방식        | 단순 규칙 기반 처리 (play + -ing → play)                             | 사전 기반 의미 분석 (better → good)                            |\n",
    "| 정확도           | 비교적 낮음 (비정상 단어 가능)                         | 높음 (정상 단어 반환)                                 |\n",
    "| 속도             | 빠름                                                   | 상대적으로 느림                                       |\n",
    "| 주요 도구        | `PorterStemmer`, `LancasterStemmer`                   | `WordNetLemmatizer`                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **어간 추출(PorterStemmer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"playing\", \"played\", \"plays\", \"player\", \"better\", \"studies\"]\n",
    "\n",
    "print(\"Stemming 결과:\")\n",
    "for word in words:\n",
    "    print(f\"{word} → {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **표제어 추출(WordNetLemmatizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\nLemmatization 결과:\")\n",
    "for word in words:\n",
    "    print(f\"{word} → {lemmatizer.lemmatize(word, pos='v')}\")  # 기본은 동사(pos='v') 기준으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization 결과:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLemmatization 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word,\u001b[38;5;250m \u001b[39mpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\nLemmatization 결과:\")\n",
    "for word in words:\n",
    "    print(f\"{word} → {lemmatizer.lemmatize(word, pos='a')}\")  # 형용사 지정(pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_no_html = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    expanded_text = contractions.fix(text_no_html)\n",
    "    \n",
    "    text_no_spectals = re.sub(r'[^a-zA-Z]', ' ', expanded_text)\n",
    "    \n",
    "    text_clean=re.sub(r'\\s+', ' ', text_no_spectals).strip()\n",
    "    \n",
    "    text_lower = text_clean.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text_lower)\n",
    "    filltered_tokecs = [word_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(preprocess_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df['text'].progress_apply(preprocess_text) #전처리 함수 적용용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 전통적인 특성 추출(텍스트 표현) 기법**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5-1. 원-핫 인코딩 (One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 단어를 고유한 인덱스에 매핑하고, 해당 인덱스만 1이고 나머지는 0인 벡터로 표현\n",
    "    \n",
    "    * 자연어 전처리에서 자주 사용되나, 딥러닝에서는 보다 효율적인 임베딩 기법으로 대체되는 추세\n",
    "    \n",
    "    * 장점: 구현이 간단하고 직관적 → 예: 단어 집합 크기가 10,000이면 각 단어는 10,000차원의 벡터 \n",
    "    \n",
    "    * 단점: ① 벡터가 매우 고차원이며 희소함 ② 단어 간 의미적 유사성이나 문맥을 반영하지 못함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 원-핫 인코딩 (One-Hot Encoding) 예시\n",
    "\n",
    "    * 단어 집합 (Vocabulary): [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
    "\n",
    "        | 단어   | 원-핫 인코딩 벡터     |\n",
    "        |--------|------------------------|\n",
    "        | apple  | [1, 0, 0, 0]           |\n",
    "        | banana | [0, 1, 0, 0]           |\n",
    "        | cherry | [0, 0, 1, 0]           |\n",
    "        | date   | [0, 0, 0, 1]           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예시 1: Keras의 `Tokenizer`를 이용한 원-핫 인코딩\n",
    "\n",
    "    * `texts_to_matrix(..., mode='binary')`는 단어가 등장했는지 여부만 반영 (1 또는 0)\n",
    "\n",
    "    * Keras의 Tokenizer는 단어 인덱스를 자동 생성하며, 딥러닝 모델 입력 형태에 적합하게 전처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 예제 텍스트 데이터\u001b[39;00m\n\u001b[0;32m      4\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love natural language processing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage models are interesting\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 예제 텍스트 데이터\n",
    "texts = [\"I love natural language processing\", \"Language models are interesting\"]\n",
    "\n",
    "# Tokenizer 인스턴스 생성 및 단어 사전 구축\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 원-핫 인코딩 수행 (binary mode)\n",
    "one_hot_results = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Word Index = \", tokenizer.word_index)\n",
    "print(\"One-Hot Encoded Vector = \\n\", one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예시 2: Scikit-learn의 `CountVectorizer`를 이용한 원-핫 인코딩\n",
    "\n",
    "    * `binary=True`: 단어 등장 여부만 반영 (TF-IDF 대신 0/1 표현)\n",
    "\n",
    "    * Scikit-learn은 전통적인 ML 모델 (로지스틱 회귀, 나이브베이즈 등)에 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['are' 'interesting' 'language' 'love' 'models' 'natural' 'processing']\n",
      "One-Hot Encoding Matrix:\n",
      " [[0 0 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 예제 텍스트 데이터\n",
    "texts = [\"I love natural language processing\", \"Language models are interesting\"]\n",
    "\n",
    "# CountVectorizer로 원-핫 인코딩 수행 (binary=True 옵션)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "one_hot_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 결과 출력\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"One-Hot Encoding Matrix:\\n\", one_hot_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5-2. TF-IDF (Term Frequency-Inverse Document Frequency))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어의 중요도를 문서 내 빈도(TF)와 전체 문서에서의 희귀도(IDF)를 통해 가중치로 표현 \n",
    "    \n",
    "    * 장점: 단순한 빈도수보다 단어의 상대적 중요도를 반영 → 자주 등장하지만 거의 모든 문서에 있는 단어는 낮은 가중치를 가짐\n",
    "    \n",
    "    * 단점: ① 단어 간 의미나 문맥 정보는 반영하지 못함 ② 벡터가 여전히 희소하고 고차원임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF 계산 예시\n",
    "  - TF-IDF는 단어의 문서 내 중요도를 수치화하는 기법으로, 다음 두 값을 곱해서 계산함\n",
    "    $$ \\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t) $$\n",
    "\n",
    "  - **TF (Term Frequency)**: 특정 문서에서 단어가 얼마나 자주 등장하는지를 나타냄  \n",
    "    $$ TF(t, d) = \\frac{\\text{단어 } t \\text{의 빈도}}{\\text{문서 } d \\text{의 전체 단어 수}} $$\n",
    "\n",
    "  - **IDF (Inverse Document Frequency)**: 전체 문서 중 특정 단어가 등장한 문서 수의 역수  \n",
    "    $$ IDF(t) = \\log \\left( \\frac{N}{df(t)} \\right) \\quad \\text{(N: 전체 문서 수, df(t): 단어 t가 등장한 문서 수)} $$\n",
    "\n",
    "\n",
    "* 예시 문서 집합\n",
    "  * 단어 집합 (Vocabulary): `[\"apple\", \"banana\", \"cherry\"]`  \n",
    "  * 전체 문서 수 \\(N = 3\\)\n",
    "\n",
    "    | 문서 ID | 내용                                  |\n",
    "    |---------|---------------------------------------|\n",
    "    | D1      | \"apple apple apple banana\"            |\n",
    "    | D2      | \"banana cherry banana banana\"         |\n",
    "    | D3      | \"cherry cherry cherry apple banana\"   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1: TF 계산\n",
    "\n",
    "    | 단어   | D1 (TF)           | D2 (TF)           | D3 (TF)           |\n",
    "    |--------|-------------------|-------------------|-------------------|\n",
    "    | apple  | 3 / 4 = 0.750     | 0 / 4 = 0.000     | 1 / 5 = 0.200     |\n",
    "    | banana | 1 / 4 = 0.250     | 3 / 4 = 0.750     | 1 / 5 = 0.200     |\n",
    "    | cherry | 0 / 4 = 0.000     | 1 / 4 = 0.250     | 3 / 5 = 0.600     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: IDF 계산\n",
    "    - apple → 2개 문서(D1, D3)  \n",
    "    - banana → 3개 문서(D1, D2, D3)  \n",
    "    - cherry → 2개 문서(D2, D3)\n",
    "\n",
    "* 로그 밑은 자연로그(ln) 또는 밑10(log₁₀) 사용 가능. 여기선 log₁₀ 기준\n",
    "\n",
    "    | 단어   | IDF 계산식              | IDF 값 (log₁₀ 기준) |\n",
    "    |--------|--------------------------|----------------------|\n",
    "    | apple  | log(3 / 2)               | ≈ 0.176              |\n",
    "    | banana | log(3 / 3) = log(1)      | = 0.000              |\n",
    "    | cherry | log(3 / 2)               | ≈ 0.176              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: TF-IDF 계산\n",
    "\n",
    "    | 단어   | D1 (TF-IDF)         | D2 (TF-IDF)         | D3 (TF-IDF)         |\n",
    "    |--------|----------------------|----------------------|----------------------|\n",
    "    | apple  | 0.750 × 0.176 ≈ 0.132 | 0.000 × 0.176 = 0.000 | 0.200 × 0.176 ≈ 0.035 |\n",
    "    | banana | 0.250 × 0.000 = 0.000 | 0.750 × 0.000 = 0.000 | 0.200 × 0.000 = 0.000 |\n",
    "    | cherry | 0.000 × 0.176 = 0.000 | 0.250 × 0.176 ≈ 0.044 | 0.600 × 0.176 ≈ 0.106 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최종 TF-IDF 행렬\n",
    "    - TF는 문서 내 상대적 빈도, IDF는 전체 문서에서의 희귀성을 반영함  \n",
    "    - banana는 모든 문서에 등장하므로 IDF=0 → 중요하지 않은 단어로 간주됨  \n",
    "    - apple, cherry는 특정 문서에 많이 등장하면서 전체 문서에서는 제한적으로 등장 → 높은 TF-IDF 부여\n",
    "\n",
    "    | 문서 ID | apple  | banana | cherry |\n",
    "    |---------|--------|--------|--------|\n",
    "    | D1      | 0.132  | 0.000  | 0.000  |\n",
    "    | D2      | 0.000  | 0.000  | 0.044  |\n",
    "    | D3      | 0.035  | 0.000  | 0.106  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF 실습 예제: Scikit-learn 활용\n",
    "\n",
    "    * Scikit-learn의 TfidfVectorizer를 사용하여 간편하게 구현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1: 예제 문서 집합 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 문서 집합\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The cat sat on the dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Step 2: CountVectorizer로 DTM 생성 (단순 단어 빈도 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# CountVectorizer 초기화 및 DTM 생성\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# numpy 배열로 변환\n",
    "count_matrix_array = count_matrix.toarray()\n",
    "\n",
    "# DataFrame 변환 및 출력\n",
    "count_df = pd.DataFrame(count_matrix_array, columns=count_vectorizer.get_feature_names_out())\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: TfidfVectorizer로 TF-IDF 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 문서 집합에 대해 TF-IDF 계산\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# TF-IDF 행렬을 배열로 변환\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "\n",
    "# 단어 리스트 추출\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame 변환 및 출력\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_array, columns=feature_names)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: TF-IDF 행렬 출력 (배열 형태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
