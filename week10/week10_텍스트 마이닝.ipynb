{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 10: í…ìŠ¤íŠ¸ ë§ˆì´ë‹**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë€?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìì—°ì–´ë¡œ ì‘ì„±ëœ ë°ì´í„°ë¥¼ ê¸°ê³„ í•™ìŠµ ë° ë¶„ì„ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ì •ì œ(Cleaning)í•˜ê³  êµ¬ì¡°í™”(Structuring)í•˜ëŠ” ì‘ì—…ì„\n",
    "  \n",
    "  * ì˜ˆ: íŠ¹ìˆ˜ë¬¸ì ì œê±°, ëŒ€ì†Œë¬¸ì í†µì¼, ë‹¨ì–´ ë¶„ë¦¬, ë¶ˆìš©ì–´ ì œê±° ë“±\n",
    "  \n",
    "  * ì¼ë°˜ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •í˜•(structured) ë°ì´í„° í˜•íƒœë¡œ ë°”ê¾¸ê¸° ìœ„í•œ ê¸°ì´ˆ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-2. ìì—°ì–´ ì²˜ë¦¬ vs í…ìŠ¤íŠ¸ ë§ˆì´ë‹**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog-ko.superb-ai.com/content/images/size/w1000/2023/07/pasted-image-0.png\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| í•­ëª©       | ìì—°ì–´ ì²˜ë¦¬ (NLP)                                  | í…ìŠ¤íŠ¸ ë§ˆì´ë‹ (Text Mining)                                 |\n",
    "|------------|----------------------------------------------------|--------------------------------------------------------------|\n",
    "| ì •ì˜       | ì¸ê°„ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ                   | í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ ì •ë³´ë‚˜ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ          |\n",
    "| ëª©ì        | ì–¸ì–´ì˜ êµ¬ì¡°ì™€ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ì—¬ ì´í•´                  | ë°ì´í„°ë¡œë¶€í„° ì¸ì‚¬ì´íŠ¸ì™€ ì§€ì‹ì„ ë°œê²¬                         |\n",
    "| ì£¼ìš” ê¸°ìˆ   | êµ¬ë¬¸ ë¶„ì„, ì˜ë¯¸ ë¶„ì„, ì–¸ì–´ ëª¨ë¸ë§ ë“±                | í…ìŠ¤íŠ¸ ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§, ê°ì„± ë¶„ì„, í† í”½ ëª¨ë¸ë§ ë“±           |\n",
    "| ì‘ìš© ë¶„ì•¼  | ì±—ë´‡, ìŒì„± ì¸ì‹, ê¸°ê³„ ë²ˆì—­ ë“±                       | ë¬¸ì„œ ë¶„ë¥˜, ì—¬ë¡  ë¶„ì„, ê¸°ì—… ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë“±                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•„ìš”ì„±**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ë°ì´í„° ì¼ê´€ì„± ìœ ì§€**\n",
    "  * ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ëŠ” í˜•ì‹, êµ¬ì¡°, í‘œí˜„ ë°©ì‹ì´ ì œê°ê¸° ë‹¤ë¥¼ ìˆ˜ ìˆìŒ â†’ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì¼ê´€ë˜ê²Œ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•  ìˆ˜ ìˆìŒ\n",
    "    * ì˜ˆ: ëŒ€ì†Œë¬¸ì í˜¼ìš©, íŠ¹ìˆ˜ ë¬¸ì, ë¶ˆí•„ìš”í•œ ê³µë°± ë“±\n",
    "  \n",
    "\n",
    "* **ë…¸ì´ì¦ˆ ì œê±°**\n",
    "  * í…ìŠ¤íŠ¸ì—ëŠ” ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°©í•´ê°€ ë˜ëŠ” ì •ë³´ë“¤ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ â†’ ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ\n",
    "    * ì˜ˆ: HTML íƒœê·¸, ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì, ê´‘ê³  ë¬¸êµ¬, ë°˜ë³µ ë¬¸ì, ì˜¤íƒ€ ë“±\n",
    "\n",
    "\n",
    "* **ì¤‘ìš” ì •ë³´ ê°•ì¡°**\n",
    "  * ë¶ˆìš©ì–´(stopwords) ì œê±°, ì–´ê°„ ì¶”ì¶œ(stemming), ì›í˜• ë³µì›(lemmatization) ë“±ì˜ ê¸°ë²•ì„ í†µí•´ í…ìŠ¤íŠ¸ ë‚´ í•µì‹¬ ì •ë³´ë¥¼ ê°•ì¡°í•  ìˆ˜ ìˆìŒ â†’ ë¶„ì„ì˜ ì •í™•ë„ì™€ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŒ\n",
    "\n",
    "\n",
    "* **ì°¨ì› ì¶•ì†Œ**\n",
    "  * ì›ì‹œ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì–´ ìˆ˜ê°€ ë§ì•„ ë²¡í„°í™” ì‹œ ë§¤ìš° ê³ ì°¨ì› ë°ì´í„°ê°€ ìƒì„±ë¨\n",
    "  \n",
    "  * ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë¥¼ ì œê±°í•˜ê±°ë‚˜ ë‹¨ì–´ë¥¼ í†µí•©í•¨ìœ¼ë¡œì¨ ì°¨ì›ì„ ì¤„ì´ê³  ê³„ì‚° ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŒ â†’ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ì™€ ì„±ëŠ¥ ê°œì„ ì— ê¸°ì—¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ vs íŠ¹ì„± ì¶”ì¶œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| í•­ëª©       | í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬                             | íŠ¹ì„± ì¶”ì¶œ                             |\n",
    "|------------|--------------------------------------------|-----------------------------------------------|\n",
    "| ëª©ì        | ëª¨ë¸ ì…ë ¥ì— ì í•©í•œ í…ìŠ¤íŠ¸ ì •ì œ             | ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ ì˜ë¯¸í•œ íŠ¹ì„± ìƒì„±     |\n",
    "| ì²˜ë¦¬ ëŒ€ìƒ  | ë‹¨ì–´, ë¬¸ì¥, íŠ¹ìˆ˜ê¸°í˜¸, ë¬¸ë²•ì  í˜•íƒœ ë“±       | í…ìŠ¤íŠ¸ ì„ë² ë”©, TF-IDF, ë¬¸ì¥ ê¸¸ì´ ë“±          |\n",
    "| ì˜ˆì‹œ       | `\"What is this???\" â†’ \"what is this\"`       | BERT ì„ë² ë”© ì¶”ì¶œ, ê°ì„± ì ìˆ˜ ìƒì„± ë“±           |\n",
    "| ì‹œì        | ëª¨ë¸ í•™ìŠµ ì „ ë‹¨ê³„                          | ì „ì²˜ë¦¬ ì´í›„, íŠ¹ì„± ì„¤ê³„ ì‹œ                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„ ë˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì— ì í•©í•œ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ê³¼ì •ì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒì˜ 3ë‹¨ê³„ë¡œ ìˆ˜í–‰ë¨\n",
    "\n",
    "* ë‹¨ê³„ë³„ ì„¸ë¶€ ì‘ì—…ì€ í”„ë¡œì íŠ¸ì˜ ëª©ì , ë°ì´í„°ì˜ íŠ¹ì„±, ë¶„ì„ ë°©í–¥ì— ë”°ë¼ ì¡°ì •í•˜ê±°ë‚˜ ìƒëµë  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-1. ë°ì´í„° ì •ì œ (Cleaning)**\n",
    "\n",
    "* ëª©ì : ë…¸ì´ì¦ˆ ì œê±° ë° ì¼ê´€ì„± í™•ë³´ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒ\n",
    "\n",
    "* ì£¼ìš” ì‘ì—…\n",
    "    \n",
    "    * ë…¸ì´ì¦ˆ ì œê±°: HTML íƒœê·¸, íŠ¹ìˆ˜ ë¬¸ì, ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ê³µë°± ë“± ì œê±°\n",
    "\n",
    "    * ëŒ€ì†Œë¬¸ì ë³€í™˜: `\"Apple\"` â†’ `\"apple\"`\n",
    "\n",
    "    * ì¶•ì•½ì–´ í™•ì¥: `\"don't\"` â†’ `\"do not\"`\n",
    "\n",
    "    * í…ìŠ¤íŠ¸ ì •ê·œí™”: `\"U.S.A.\"` â†’ `\"USA\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-2. ë°ì´í„° ë³€í™˜ (Transformation)**\n",
    "\n",
    "* ëª©ì : í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„/ëª¨ë¸ë§ì— ì í•©í•œ í˜•íƒœë¡œ êµ¬ì¡°í™”\n",
    "\n",
    "* ì£¼ìš” ì‘ì—…\n",
    "    \n",
    "    * í† í°í™”(Tokenization): ë¬¸ì¥ì„ ë‹¨ì–´ ë˜ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "\n",
    "    * ë¶ˆìš©ì–´ ì œê±°(Stopword Removal): ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: the, and, is ë“±)\n",
    "\n",
    "    * ì–´ê°„ ì¶”ì¶œ(Stemming) ë˜ëŠ” ì›í˜• ë³µì›(Lemmatization):\n",
    "\n",
    "        * `\"running\"` â†’ `\"run\"`\n",
    "\n",
    "        * `\"better\"` â†’ `\"good\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-3. íŠ¹ì„± ì¶”ì¶œ (Feature Extraction)**\n",
    "\n",
    "* ëª©ì : í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë¸ í•™ìŠµì— í™œìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜• íŠ¹ì„±(Feature) ì¶”ì¶œ\n",
    "\n",
    "* ì£¼ìš” ì‘ì—…\n",
    "    \n",
    "    * ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ í‘œí˜„: One-hot Encoding, TF-IDF ë“±ìœ¼ë¡œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ ìˆ˜ì¹˜í™”\n",
    "\n",
    "    * ë‹¨ì–´ ì„ë² ë”©(Word Embedding): ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ìƒ ìœ„ì¹˜ë¡œ í‘œí˜„ (ì˜ˆ: Word2Vec, GloVe, BERT Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. í…ìŠ¤íŠ¸ ì •ì œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-1. HTML íƒœê·¸ ì œê±°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì›¹ í¬ë¡¤ë§ ë°ì´í„°ì—ëŠ” ë‹¤ì–‘í•œí•œ HTML íƒœê·¸ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°,ì´ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ì—ëŠ” ë¶ˆí•„ìš”í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    <html>\n",
    "        <head><title>My Title</title></head> ğŸ˜ğŸ˜\n",
    "        <body><p>Hello, World! &#128512;</p><p>Python is fun. <a href=\"http://example.com\">Example</a></p></body>\n",
    "    </html>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     \n",
      "          My Title   ğŸ˜ğŸ˜\n",
      "          Hello, World! &#128512;  Python is fun.  Example   \n",
      "     \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# re.sub í•¨ìˆ˜\n",
    "text_no_html = re.sub(r'<.*?>', ' ', text) #ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ë¦¬\n",
    "print(text_no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My Title ğŸ˜ğŸ˜\n",
      "        Hello, World! ğŸ˜€Python is fun. Example\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# BeautifulSoup\n",
    "text_no_html = BeautifulSoup(text, \"html.parser\").get_text() #ì´ê±°ëŠ” ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì •ë¦¬ ì•ˆí•´ë„ ë¨\n",
    "print(text_no_html)  #ì“¸ë–„ì—†ëŠ”ê±¸ ì—†ì• ì¤Œ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.sub()` vs `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| í•­ëª©             | `re.sub()`                                          | `BeautifulSoup`                                                  |\n",
    "|------------------|-----------------------------------------------------|------------------------------------------------------------------|\n",
    "| ì£¼ìš” ëª©ì         | ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ ë‚´ íŒ¨í„´ì„ ì¹˜í™˜ ë˜ëŠ” ì œê±° | HTML/XML êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ì—¬ ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ì œê±°                  |\n",
    "| ì‚¬ìš© ë°©ì‹        | ë¬¸ìì—´ ê¸°ë°˜ ì²˜ë¦¬                                     | íŠ¸ë¦¬ êµ¬ì¡° ê¸°ë°˜ ì²˜ë¦¬ (DOM íƒìƒ‰ ê°€ëŠ¥)                              |\n",
    "| HTML ì²˜ë¦¬ ì í•©ì„± | ë³µì¡í•œ HTMLì€ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì›€                        | HTML íƒœê·¸, ì†ì„± ë“±ì„ êµ¬ì¡°ì ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì²˜ë¦¬ ê°€ëŠ¥              |\n",
    "| í•™ìŠµ ë‚œì´ë„      | ì •ê·œí‘œí˜„ì‹ ë¬¸ë²• ìˆ™ì§€ê°€ í•„ìš”                          | ë¹„êµì  ì§ê´€ì ì¸ API (`.find()`, `.text` ë“±) ì œê³µ                 |\n",
    "| ì†ë„             | ë¹ ë¦„ (ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¹˜í™˜ ì‹œ)                          | ë‹¤ì†Œ ëŠë¦¼ (HTML íŒŒì‹± í¬í•¨)                                       |\n",
    "| ì£¼ìš” í™œìš© ì˜ˆ     | ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì •ì œ (ì˜ˆ: íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì´ë©”ì¼ ìˆ˜ì •) | HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ, íƒœê·¸ ì œê±°, ì†ì„± ì ‘ê·¼ ë“± ì›¹ í¬ë¡¤ë§ì— ì í•©   |\n",
    "| ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš” | í•„ìš” ì—†ìŒ (`re`ëŠ” íŒŒì´ì¬ ë‚´ì¥ ëª¨ë“ˆ)               | í•„ìš” ìˆìŒ (`bs4` íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”: `pip install beautifulsoup4`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-2. ì´ëª¨ì§€ ì œê±°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì´ëª¨ì§€ëŠ” í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ìœ ë‹ˆì½”ë“œ íŠ¹ìˆ˜ë¬¸ìì´ê¸° ë•Œë¬¸ì— ì¼ë°˜ NLP ì²˜ë¦¬ ì‹œ ì œê±° í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love it ğŸ˜ğŸ˜!! Best product ever!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love it !! Best product ever!!!\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-3. íŠ¹ìˆ˜ë¬¸ì ì œê±°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë¬¸ì¥ ë¶€í˜¸ë‚˜ ê¸°í˜¸ ë“±ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¶„ì„ì— ë„ì›€ì´ ë˜ì§€ ì•Šìœ¼ë©°, ë‹¨ì–´ í† í°í™”ì— í˜¼ë€ì„ ì¤„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "* ë‹¤ë§Œ, ê°ì„± ë¶„ì„ ë“± íŠ¹ì • ìƒí™©ì—ì„œëŠ” ìœ ì§€í•  ìˆ˜ë„ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I can't believe this!!! It's amazing :) #excited ğŸ˜ğŸ˜\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cant believe this Its amazing  excited \n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-4. ìˆ«ì ì œê±°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í…ìŠ¤íŠ¸ ë¶„ì„ ëª©ì ì— ë”°ë¼ ìˆ«ìëŠ” ì˜ë¯¸ê°€ ì—†ê±°ë‚˜ ì˜¤íˆë ¤ ë…¸ì´ì¦ˆê°€ ë  ìˆ˜ ìˆìŒ\n",
    "\n",
    "* ì˜ˆ: ê°€ê²©, ë‚ ì§œ, ìˆ«ìí˜• ì½”ë“œ ë“± ì œê±° í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This phone costs 999 dollars and was released in 2023.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This phone costs  dollars and was released in .\n"
     ]
    }
   ],
   "source": [
    "cleaned = re.sub(r'\\d+', '', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-5. ì†Œë¬¸ì ë³€í™˜**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” `'Apple'`ê³¼ `'apple'`ì„ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ë‹¨ì–´ë¡œ ì·¨ê¸‰í•´ì•¼ í•  ê²½ìš°ê°€ ë§ìŒ\n",
    "\n",
    "* ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ” ì²˜ë¦¬ëŠ” ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ìˆ˜ ì¦ê°€ë¥¼ ë°©ì§€í•˜ê³  í•™ìŠµ ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ë†’ì—¬ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is Looking at Buying Another Startup.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple is looking at buying another startup.\n"
     ]
    }
   ],
   "source": [
    "lowered = text.lower()\n",
    "print(lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-6. ê³µë°± ì²˜ë¦¬** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* í…ìŠ¤íŠ¸ì—ëŠ” ì˜ë„ì¹˜ ì•Šì€ ì—¬ëŸ¬ ê°œì˜ ê³µë°±, íƒ­, ì¤„ë°”ê¿ˆ ë¬¸ì ë“±ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ\n",
    "\n",
    "* ì´ëŸ¬í•œ ê³µë°±ì€ í† í°í™” ë˜ëŠ” ëª¨ë¸ ì…ë ¥ ì‹œ ë¶ˆí•„ìš”í•œ í† í° ìƒì„±ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "     \n",
    "          My Title  \n",
    "          Hello World   Python is fun  Example   \n",
    "     \n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Title Hello World Python is fun Example\n"
     ]
    }
   ],
   "source": [
    "text_clean = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-7. ì¶•ì•½ì–´ í™•ì¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì¶•ì•½ì–´ í™•ì¥ì€ `'don't'`ë¥¼ `'do not'`ìœ¼ë¡œ, `'I'm'`ì„ `'I am'`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë“± ì¶•ì•½ëœ ë‹¨ì–´ í˜•íƒœë¥¼ ì›ë˜ í˜•íƒœë¡œ í™•ì¥í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•¨\n",
    "\n",
    "* ì¶•ì•½í˜•ì„ ì›í˜•ìœ¼ë¡œ í™•ì¥í•˜ë©´ ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ë™ì¼í•œ í˜•íƒœë¥¼ ê°–ê²Œ ë˜ì–´ ë¶„ì„ì˜ ì¼ê´€ì„±ì´ í–¥ìƒí•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install contractions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m   \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ì¶•ì•½í˜•ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸\u001b[39;00m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm here but I don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know what to do.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "#!pip install contractions\n",
    "import contractions   \n",
    "\n",
    "# ì¶•ì•½í˜•ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸\n",
    "text = \"I'm here but I don't know what to do.\"\n",
    "\n",
    "# ì¶•ì•½í˜• í™•ì¥\n",
    "expanded_text = contractions.fix(text)\n",
    "\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. ë°ì´í„° ë³€í™˜**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-1. í† í°í™”**\n",
    "\n",
    "* ë¬¸ì¥ì„ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ì¸ ë‹¨ì–´(word), í˜•íƒœì†Œ(morpheme), ì„œë¸Œì›Œë“œ(subword) ë“±ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì„\n",
    "\n",
    "* BoW, TF-IDF, ì›Œë“œ ì„ë² ë”© ë“± ëª¨ë‘ í† í°í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLTK (Natural Language Toolkit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLTKëŠ” Pythonì—ì„œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ëŒ€í‘œì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì„\n",
    "\n",
    "* í† í°í™”(tokenization), í˜•íƒœì†Œ ë¶„ì„(pos tagging), ë¶ˆìš©ì–´ ì œê±°, ì–´ê°„ ì¶”ì¶œ ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•¨\n",
    "\n",
    "* ì‹¤ë¬´ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ì†ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°ì—ëŠ” spaCy, Hugging Face Transformersê°€ ë” ì í•©í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')  \n",
    "# nltk.download('punkt_tab') # í† í°í™” ë„êµ¬(ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ í•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UserK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is fun!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-2. ë¶ˆìš©ì–´ ì œê±°**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë¬¸ì¥ì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ë¬¸ë§¥ìƒ ì˜ë¯¸ê°€ ê±°ì˜ ì—†ê±°ë‚˜ ë¶„ì„ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë¥¼ ì˜ë¯¸í•¨\n",
    "\n",
    "* ì˜ˆ: `is`, `the`, `a`, `and`, `in`, `of`,` to`, `this`, `that` ë“±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')   # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸(ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ í•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UserK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence with some stopwords.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# ë¶ˆìš©ì–´ ì œê±°\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì‚¬ì „ êµ¬ì¶• ë°©ë²•**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì‚¬ìš© ë„ë©”ì¸ì— ë”°ë¼ ê¸°ë³¸ ë¶ˆìš©ì–´ ì™¸ì— ì¶”ê°€/ì œê±°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "* ì˜ˆ: ê¸ˆìœµ ë„ë©”ì¸ì—ì„œ â€˜bankâ€™ëŠ” ë¶ˆìš©ì–´ê°€ ì•„ë‹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n",
    "custom_stopwords = set(stopwords.words('english'))\n",
    "custom_stopwords.remove('not')                     # ì œê±°: 'not'ì€ ë¶„ì„ì— ì¤‘ìš”í•  ìˆ˜ ìˆìŒ\n",
    "custom_stopwords.update(['sample', 'sentence'])    # ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['than', \"you'll\", 'further', 'into', 'a', 'on', 'over', 'how', \"they'll\", \"it'll\", 'm', 'o', 'as', 'only', 'both', 'ma', 'whom', 'my', 'with', 'do']\n"
     ]
    }
   ],
   "source": [
    "# ì¼ë¶€ ë¶ˆìš©ì–´ ì¶œë ¥\n",
    "print(list(stop_words)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ìš©\n",
    "filtered = [word for word in tokens if word not in custom_stopwords]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-3. ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| í•­ëª©             | Stemming (ì–´ê°„ ì¶”ì¶œ)                                  | Lemmatization (í‘œì œì–´ ì¶”ì¶œ)                           |\n",
    "|------------------|--------------------------------------------------------|--------------------------------------------------------|\n",
    "| ì •ì˜             | \të‹¨ì–´ì˜ ì ‘ì‚¬ ë“±ì„ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì œê±°í•˜ì—¬ ì–´ê·¼ ì¶”ì¶œ | ë¬¸ë²•ì  í’ˆì‚¬ì™€ ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ í‘œì œì–´(ê¸°ë³¸í˜•)ë¡œ ë³€í™˜ |\n",
    "| ì²˜ë¦¬ ë°©ì‹        | ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬ (play + -ing â†’ play)                             | ì‚¬ì „ ê¸°ë°˜ ì˜ë¯¸ ë¶„ì„ (better â†’ good)                            |\n",
    "| ì •í™•ë„           | ë¹„êµì  ë‚®ìŒ (ë¹„ì •ìƒ ë‹¨ì–´ ê°€ëŠ¥)                         | ë†’ìŒ (ì •ìƒ ë‹¨ì–´ ë°˜í™˜)                                 |\n",
    "| ì†ë„             | ë¹ ë¦„                                                   | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼                                       |\n",
    "| ì£¼ìš” ë„êµ¬        | `PorterStemmer`, `LancasterStemmer`                   | `WordNetLemmatizer`                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ì–´ê°„ ì¶”ì¶œ(PorterStemmer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"playing\", \"played\", \"plays\", \"player\", \"better\", \"studies\"]\n",
    "\n",
    "print(\"Stemming ê²°ê³¼:\")\n",
    "for word in words:\n",
    "    print(f\"{word} â†’ {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **í‘œì œì–´ ì¶”ì¶œ(WordNetLemmatizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\nLemmatization ê²°ê³¼:\")\n",
    "for word in words:\n",
    "    print(f\"{word} â†’ {lemmatizer.lemmatize(word, pos='v')}\")  # ê¸°ë³¸ì€ ë™ì‚¬(pos='v') ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization ê²°ê³¼:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLemmatization ê²°ê³¼:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m â†’ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word,\u001b[38;5;250m \u001b[39mpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\nLemmatization ê²°ê³¼:\")\n",
    "for word in words:\n",
    "    print(f\"{word} â†’ {lemmatizer.lemmatize(word, pos='a')}\")  # í˜•ìš©ì‚¬ ì§€ì •(pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_no_html = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    expanded_text = contractions.fix(text_no_html)\n",
    "    \n",
    "    text_no_spectals = re.sub(r'[^a-zA-Z]', ' ', expanded_text)\n",
    "    \n",
    "    text_clean=re.sub(r'\\s+', ' ', text_no_spectals).strip()\n",
    "    \n",
    "    text_lower = text_clean.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text_lower)\n",
    "    filltered_tokecs = [word_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(preprocess_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df['text'].progress_apply(preprocess_text) #ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. ì „í†µì ì¸ íŠ¹ì„± ì¶”ì¶œ(í…ìŠ¤íŠ¸ í‘œí˜„) ê¸°ë²•**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5-1. ì›-í•« ì¸ì½”ë”© (One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì¸ë±ìŠ¤ì— ë§¤í•‘í•˜ê³ , í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ ë²¡í„°ë¡œ í‘œí˜„\n",
    "    \n",
    "    * ìì—°ì–´ ì „ì²˜ë¦¬ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ë‚˜, ë”¥ëŸ¬ë‹ì—ì„œëŠ” ë³´ë‹¤ íš¨ìœ¨ì ì¸ ì„ë² ë”© ê¸°ë²•ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” ì¶”ì„¸\n",
    "    \n",
    "    * ì¥ì : êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì§ê´€ì  â†’ ì˜ˆ: ë‹¨ì–´ ì§‘í•© í¬ê¸°ê°€ 10,000ì´ë©´ ê° ë‹¨ì–´ëŠ” 10,000ì°¨ì›ì˜ ë²¡í„° \n",
    "    \n",
    "    * ë‹¨ì : â‘  ë²¡í„°ê°€ ë§¤ìš° ê³ ì°¨ì›ì´ë©° í¬ì†Œí•¨ â‘¡ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ë‚˜ ë¬¸ë§¥ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì›-í•« ì¸ì½”ë”© (One-Hot Encoding) ì˜ˆì‹œ\n",
    "\n",
    "    * ë‹¨ì–´ ì§‘í•© (Vocabulary): [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
    "\n",
    "        | ë‹¨ì–´   | ì›-í•« ì¸ì½”ë”© ë²¡í„°     |\n",
    "        |--------|------------------------|\n",
    "        | apple  | [1, 0, 0, 0]           |\n",
    "        | banana | [0, 1, 0, 0]           |\n",
    "        | cherry | [0, 0, 1, 0]           |\n",
    "        | date   | [0, 0, 0, 1]           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì˜ˆì‹œ 1: Kerasì˜ `Tokenizer`ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”©\n",
    "\n",
    "    * `texts_to_matrix(..., mode='binary')`ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í–ˆëŠ”ì§€ ì—¬ë¶€ë§Œ ë°˜ì˜ (1 ë˜ëŠ” 0)\n",
    "\n",
    "    * Kerasì˜ TokenizerëŠ” ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ìë™ ìƒì„±í•˜ë©°, ë”¥ëŸ¬ë‹ ëª¨ë¸ ì…ë ¥ í˜•íƒœì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ì˜ˆì œ í…ìŠ¤íŠ¸ ë°ì´í„°\u001b[39;00m\n\u001b[0;32m      4\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love natural language processing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage models are interesting\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# ì˜ˆì œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "texts = [\"I love natural language processing\", \"Language models are interesting\"]\n",
    "\n",
    "# Tokenizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰ (binary mode)\n",
    "one_hot_results = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Word Index = \", tokenizer.word_index)\n",
    "print(\"One-Hot Encoded Vector = \\n\", one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì˜ˆì‹œ 2: Scikit-learnì˜ `CountVectorizer`ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”©\n",
    "\n",
    "    * `binary=True`: ë‹¨ì–´ ë“±ì¥ ì—¬ë¶€ë§Œ ë°˜ì˜ (TF-IDF ëŒ€ì‹  0/1 í‘œí˜„)\n",
    "\n",
    "    * Scikit-learnì€ ì „í†µì ì¸ ML ëª¨ë¸ (ë¡œì§€ìŠ¤í‹± íšŒê·€, ë‚˜ì´ë¸Œë² ì´ì¦ˆ ë“±)ì— ì í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['are' 'interesting' 'language' 'love' 'models' 'natural' 'processing']\n",
      "One-Hot Encoding Matrix:\n",
      " [[0 0 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ì˜ˆì œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "texts = [\"I love natural language processing\", \"Language models are interesting\"]\n",
    "\n",
    "# CountVectorizerë¡œ ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰ (binary=True ì˜µì…˜)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "one_hot_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"One-Hot Encoding Matrix:\\n\", one_hot_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5-2. TF-IDF (Term Frequency-Inverse Document Frequency))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë¬¸ì„œ ë‚´ ë¹ˆë„(TF)ì™€ ì „ì²´ ë¬¸ì„œì—ì„œì˜ í¬ê·€ë„(IDF)ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ë¡œ í‘œí˜„ \n",
    "    \n",
    "    * ì¥ì : ë‹¨ìˆœí•œ ë¹ˆë„ìˆ˜ë³´ë‹¤ ë‹¨ì–´ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ë°˜ì˜ â†’ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ê±°ì˜ ëª¨ë“  ë¬¸ì„œì— ìˆëŠ” ë‹¨ì–´ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§\n",
    "    \n",
    "    * ë‹¨ì : â‘  ë‹¨ì–´ ê°„ ì˜ë¯¸ë‚˜ ë¬¸ë§¥ ì •ë³´ëŠ” ë°˜ì˜í•˜ì§€ ëª»í•¨ â‘¡ ë²¡í„°ê°€ ì—¬ì „íˆ í¬ì†Œí•˜ê³  ê³ ì°¨ì›ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF ê³„ì‚° ì˜ˆì‹œ\n",
    "  - TF-IDFëŠ” ë‹¨ì–´ì˜ ë¬¸ì„œ ë‚´ ì¤‘ìš”ë„ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, ë‹¤ìŒ ë‘ ê°’ì„ ê³±í•´ì„œ ê³„ì‚°í•¨\n",
    "    $$ \\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t) $$\n",
    "\n",
    "  - **TF (Term Frequency)**: íŠ¹ì • ë¬¸ì„œì—ì„œ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„  \n",
    "    $$ TF(t, d) = \\frac{\\text{ë‹¨ì–´ } t \\text{ì˜ ë¹ˆë„}}{\\text{ë¬¸ì„œ } d \\text{ì˜ ì „ì²´ ë‹¨ì–´ ìˆ˜}} $$\n",
    "\n",
    "  - **IDF (Inverse Document Frequency)**: ì „ì²´ ë¬¸ì„œ ì¤‘ íŠ¹ì • ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜ì˜ ì—­ìˆ˜  \n",
    "    $$ IDF(t) = \\log \\left( \\frac{N}{df(t)} \\right) \\quad \\text{(N: ì „ì²´ ë¬¸ì„œ ìˆ˜, df(t): ë‹¨ì–´ tê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜)} $$\n",
    "\n",
    "\n",
    "* ì˜ˆì‹œ ë¬¸ì„œ ì§‘í•©\n",
    "  * ë‹¨ì–´ ì§‘í•© (Vocabulary): `[\"apple\", \"banana\", \"cherry\"]`  \n",
    "  * ì „ì²´ ë¬¸ì„œ ìˆ˜ \\(N = 3\\)\n",
    "\n",
    "    | ë¬¸ì„œ ID | ë‚´ìš©                                  |\n",
    "    |---------|---------------------------------------|\n",
    "    | D1      | \"apple apple apple banana\"            |\n",
    "    | D2      | \"banana cherry banana banana\"         |\n",
    "    | D3      | \"cherry cherry cherry apple banana\"   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1: TF ê³„ì‚°\n",
    "\n",
    "    | ë‹¨ì–´   | D1 (TF)           | D2 (TF)           | D3 (TF)           |\n",
    "    |--------|-------------------|-------------------|-------------------|\n",
    "    | apple  | 3 / 4 = 0.750     | 0 / 4 = 0.000     | 1 / 5 = 0.200     |\n",
    "    | banana | 1 / 4 = 0.250     | 3 / 4 = 0.750     | 1 / 5 = 0.200     |\n",
    "    | cherry | 0 / 4 = 0.000     | 1 / 4 = 0.250     | 3 / 5 = 0.600     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: IDF ê³„ì‚°\n",
    "    - apple â†’ 2ê°œ ë¬¸ì„œ(D1, D3)  \n",
    "    - banana â†’ 3ê°œ ë¬¸ì„œ(D1, D2, D3)  \n",
    "    - cherry â†’ 2ê°œ ë¬¸ì„œ(D2, D3)\n",
    "\n",
    "* ë¡œê·¸ ë°‘ì€ ìì—°ë¡œê·¸(ln) ë˜ëŠ” ë°‘10(logâ‚â‚€) ì‚¬ìš© ê°€ëŠ¥. ì—¬ê¸°ì„  logâ‚â‚€ ê¸°ì¤€\n",
    "\n",
    "    | ë‹¨ì–´   | IDF ê³„ì‚°ì‹              | IDF ê°’ (logâ‚â‚€ ê¸°ì¤€) |\n",
    "    |--------|--------------------------|----------------------|\n",
    "    | apple  | log(3 / 2)               | â‰ˆ 0.176              |\n",
    "    | banana | log(3 / 3) = log(1)      | = 0.000              |\n",
    "    | cherry | log(3 / 2)               | â‰ˆ 0.176              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: TF-IDF ê³„ì‚°\n",
    "\n",
    "    | ë‹¨ì–´   | D1 (TF-IDF)         | D2 (TF-IDF)         | D3 (TF-IDF)         |\n",
    "    |--------|----------------------|----------------------|----------------------|\n",
    "    | apple  | 0.750 Ã— 0.176 â‰ˆ 0.132 | 0.000 Ã— 0.176 = 0.000 | 0.200 Ã— 0.176 â‰ˆ 0.035 |\n",
    "    | banana | 0.250 Ã— 0.000 = 0.000 | 0.750 Ã— 0.000 = 0.000 | 0.200 Ã— 0.000 = 0.000 |\n",
    "    | cherry | 0.000 Ã— 0.176 = 0.000 | 0.250 Ã— 0.176 â‰ˆ 0.044 | 0.600 Ã— 0.176 â‰ˆ 0.106 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìµœì¢… TF-IDF í–‰ë ¬\n",
    "    - TFëŠ” ë¬¸ì„œ ë‚´ ìƒëŒ€ì  ë¹ˆë„, IDFëŠ” ì „ì²´ ë¬¸ì„œì—ì„œì˜ í¬ê·€ì„±ì„ ë°˜ì˜í•¨  \n",
    "    - bananaëŠ” ëª¨ë“  ë¬¸ì„œì— ë“±ì¥í•˜ë¯€ë¡œ IDF=0 â†’ ì¤‘ìš”í•˜ì§€ ì•Šì€ ë‹¨ì–´ë¡œ ê°„ì£¼ë¨  \n",
    "    - apple, cherryëŠ” íŠ¹ì • ë¬¸ì„œì— ë§ì´ ë“±ì¥í•˜ë©´ì„œ ì „ì²´ ë¬¸ì„œì—ì„œëŠ” ì œí•œì ìœ¼ë¡œ ë“±ì¥ â†’ ë†’ì€ TF-IDF ë¶€ì—¬\n",
    "\n",
    "    | ë¬¸ì„œ ID | apple  | banana | cherry |\n",
    "    |---------|--------|--------|--------|\n",
    "    | D1      | 0.132  | 0.000  | 0.000  |\n",
    "    | D2      | 0.000  | 0.000  | 0.044  |\n",
    "    | D3      | 0.035  | 0.000  | 0.106  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF ì‹¤ìŠµ ì˜ˆì œ: Scikit-learn í™œìš©\n",
    "\n",
    "    * Scikit-learnì˜ TfidfVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„í¸í•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1: ì˜ˆì œ ë¬¸ì„œ ì§‘í•© ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì œ ë¬¸ì„œ ì§‘í•©\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The cat sat on the dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Step 2: CountVectorizerë¡œ DTM ìƒì„± (ë‹¨ìˆœ ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# CountVectorizer ì´ˆê¸°í™” ë° DTM ìƒì„±\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "count_matrix_array = count_matrix.toarray()\n",
    "\n",
    "# DataFrame ë³€í™˜ ë° ì¶œë ¥\n",
    "count_df = pd.DataFrame(count_matrix_array, columns=count_vectorizer.get_feature_names_out())\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: TfidfVectorizerë¡œ TF-IDF í–‰ë ¬ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# ë¬¸ì„œ ì§‘í•©ì— ëŒ€í•´ TF-IDF ê³„ì‚°\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# TF-IDF í–‰ë ¬ì„ ë°°ì—´ë¡œ ë³€í™˜\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "\n",
    "# ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame ë³€í™˜ ë° ì¶œë ¥\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_array, columns=feature_names)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: TF-IDF í–‰ë ¬ ì¶œë ¥ (ë°°ì—´ í˜•íƒœ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
